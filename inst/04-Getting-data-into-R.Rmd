```{r echo=FALSE}
library(knitr)
opts_chunk$set(messages = FALSE,
               warning = FALSE,
               fig.align = 'center',
               fig.width = 7)
```


# Getting data into R {#gettingdatain}
  
## Overview

In this chapter we will survey some of the key methods for getting data into R. The canonical method for many years has been to use the `read.csv` function with a CSV (comma separated variables) file. This is a decent approach because most file types that contain tabular data can be saved as CSV, for example, we can save Excel files as CSV using Microsoft Excel. CSV is also ideal because it is an open, plain-text format that can be widely read and edited by many different programs. It is suitable for long-term archiving because it is a stable format. However, it is often more convienent to work directly with the file that the data were entered into, and convienent to work with a format that our collaborators are comfortable with, such as an Excel spreadsheet. Fortunately, there are several packages that allow Excel, other common tabular data file formats, and even databases, to be read directly into R. 

R is also capable of handling more challenging situations, such as working with hundreds or thousands of data files, reading tabular data from Microsoft Word documents or PDFs, and directly from the internet. In this chapter we will breifly demonstrate how to handle these situations 

## Getting data from spreadsheets

### One spreadsheet file, one or more sheets

Although the CSV file might be the ideal format for tabular data, and the simplest to work with in R, in our day-to-day work with diverse collaborators usually means that we need to get data in and out of Microsoft Excel. There are many packages for working with Excel files (e.g. gdata and xlsx), but the one I recommend is readxl by Hadley Wickham because it is the simplest to install and use. You can install it with `install.packages("readxl")`. Here is a demonstration of a simple use of this package to read in one sheet of one Excel file:

```{r readxlsimple}
library(readxl)
jerimalai_sq_a <- read_excel("data/jerimalai_lithics.xlsx")
# if you have a CSV file, use read.csv() instead of read_excel()
```

As you type that code to import your own spreadsheets, you can save some typing with a special tab-complete function in RStudio. After you type `read_excel("")`, move your cursor between the two double quotation marks, and press tab (or Control+Space). A small window will pop up at that location, giving you a list of items to choose from, saving you from typing the full names of the path and file. Figure \@ref(fig:rstudiotabcompletefilebrowsing) shows an example of this, where you can see that I have tabbed once to get a list and select the 'data' folder, and then I've tabbed again to get a list of the contents of that folder. Then I can use the arrow keys to select, or type the first few letters of the file I want to complete the command. This kind of tab-completion is possible for many R functions, and can substantially increase the efficiency of your coding by reducing the amount of typing you need to do, and saving you from making spelling mistakes. 

```{r rstudiotabcompletefilebrowsing, echo = FALSE,  out.width = "400pt", fig.cap="Schematic of file organisation for a simple example of using an Excel file with R."}
knitr::include_graphics("images/rstudio_tab_complete_file_browsing.png")
```

Before discussing the new details in the example above, let's briefly review the elements of this code that will be familiar from Chapter 1. The first line of the above code chunk, `library(readxl)`, loads the functions in the readxl package into our current R session. The second line contains a function, `read_excel()`, this does the work of extracting data from our Excel file. We also see the assignment operator, `<-`, which takes the output from the `read_excel()` function and sends it to be stored in a new object that we have decided to call `jerimalai_sq_a`. This new object is a data frame, which is similar to how our data looks when we browse it in Excel. We can see the column names are as we expect them to be. If you want to get directly to working with your data now that you've imported it, skip to Chapter XX, otherwise read on to learn more about importing data. 

In this example we have an Excel file called `jerimalai_lithics.xlsx` that is located in a folder called `data`. Recalling the advice in Chapter 1, the first step when starting a session of work on data analysis is to set our Working Directory to the 'Source file location' (go to RStudio toolbar menu, click on 'Session', then 'Set Working Directory' then 'To Source File Location'). This means that if we've followed the file organisation recommendations of Chapter 2, we have a folder called `data` in the folder that contains the source document (i.e. this document that you are reading), then we can easily connect to files in that directly. In Figure \@ref(fig:simpleexcelfileorganisation) we see a schematic of this file organisation, with the source document to the left, and the Excel file in the 'data' folder to the right.

```{r simpleexcelfileorganisation, echo = FALSE, fig.cap="Schematic of file organisation for a simple example of using an Excel file with R."}
knitr::include_graphics("images/simple_excel_file_organisation.png")
```

The Excel file in this example contains some of the data collected from excavations at Jerimalai rockshelter, East Timor [@MarwickJerimalai]. One row represents one stone artefact, and there are two sheets, one for each excavation square. To access the other sheet, we can look at the documentation for `read_excel()` by typing `?read_excel` in our console and pressing Enter. We can see in the 'Usage' and 'Arguments' sections of the documentation (Figure \@ref(fig:readexcelhelppage)) that there is an argument called `sheet` which we can use to specify what sheet in the Excel will be read into R. The text in 'Arguments' tells us that we can refer to sheets by number or the actual name of the sheet as it is in the Excel file. In the 'Usage' section we can see that the default value for `sheet` is 1. This means that if we don't specify what sheet we want, we will automatically get the first sheet in the Excel file. 

```{r readexcelhelppage, echo = FALSE, out.width = "400pt", fig.cap="Screenshot of the help page for the function read\\_excel."}
knitr::include_graphics("images/read_excel_help_page.png")
```

We can skip over most of the details on the `read_excel()` help page, but there is one more argument that is worth nothing because it is very handy. The `skip` argument allows us to specify a number of row to skip when importing from and Excel file. This is especially useful when our speadsheet has a row with merged cells at the top, perhaps to indicate groups of similiar variables (i.e. columns). We usually want to skip over these rows with merged cells when we import data from spreadsheets like this. For example, we might do `read_excel("data/jerimalai_lithics.xlsx", skip = 1)` to skip the first row of our spreadsheet (our example spreadsheet does not contain any merged cells in the first row, so we don't need to skip it for this spreadsheet).

If we wanted to read in the second sheet of our Excel file, which contains data on the stone artefacts from excavation square B, we have two options:

```{r readxlsimplesheets}
jerimalai_sq_b <- read_excel("data/jerimalai_lithics.xlsx",
                             sheet = 2) # using the sheet number

# or we can do it like this:

jerimalai_sq_b <- read_excel("data/jerimalai_lithics.xlsx",
                             sheet = "Jerimalai_All_Artefacts_Sq_B") 
# using the name of the sheet in our Excel file
```

Now we have read in two sheets from our Excel file, and we have two data frames ready to work with in our R session. But what if we have a large number of sheets in our Excel file? In this case it may be tedious to repeat these lines many times, changing the sheet name for each copy of the lines. Avoiding repititive tedium is one of the strengths of using a programmming language for data analysis, and here is how we can solve this particular problem:

```{r readmanysimpleexcelsheets}
# create a character vector of sheet names
jerimalai_sheet_names <- excel_sheets("data/jerimalai_lithics.xlsx")

# iterate over the vector of sheet names to read in the data from each sheet
library(purrr)
jerimalai_all_sheets <- map(jerimalai_sheet_names, 
                            ~read_excel("data/jerimalai_lithics.xlsx",
                                        sheet = .x))
```

In plain English, the above example uses the `excel_sheets()` function (from the readxl package) to get the sheet names from our Excel file. We have just two sheets in our example, but this same approach could be used for 10, or 100, or more sheets. The object `jerimalai_sheet_names` is a character vector containing the names of the sheets in our Excel file. The next part of the example loads functions from the purrr package (Hadley Wickham), and then shows how we can use the `map()` function to apply to the `read_excel()` function to each sheet of our Excel file. For each item in the `jerimalai_sheet_names` vector, that is, each sheet name, the `map()` function will apply the `read_excel()` function, and store the resulting data frame in `jerimalai_all_sheets`. There are two important details in the `map()` function, first is the `~`, which we use to indicate that we want the function `read_excel()` to operate on each element of `jerimalai_sheet_names`, and second is the `.x` which is a place-holder for each specific element of `jerimalai_sheet_names`.

In the above example we take advantage of a very powerful and efficient programming concept known as a loop or for-loop. This is an instruction to the computer to repeat a piece of code a specific number of times. In our example, the computer will repeat `read_excel()` two times, once for each sheet in our Excel file. During the first repitition, the `.x` in the `map()` function takes the value of `r jerimalai_sheet_names[1]`, the first sheet name, and during the second repitition the `.x` becomes `r jerimalai_sheet_names[2]`, the second sheet name. With just two repititions, this complexity might seem like overkill, but you can imagine how much time we would save if we had 50 or 500 sheets. There are other methods for writing for loops in R (e.g. using `for()` and `lapply()`), but I prefer `map()` for most occasions because it is quick to type and easy to read and understand when I come back to my code after some time away and can't immediately remember what is going on. For further reading about other methods of writing loops in R, I recommend de Vries and Meys [-@de2015r], Matloff [-@matloff2011art], or Wickham [-@wickham2014advanced].

The result of our `map()` function is a list of two data frames. A list is a very useful data type in R, and worth to get comfortable with. So far we have used vectors (a single sequence of data elements), such as the character vector `jerimalai_sheet_names` that holds the sheet names of our Excel file, and data frames, which are tables of data similar to what we see in Excel spreadsheets and other tabular structures. Often our data frames have one observation per row, with each column representing a variable, a measure, feature, or
characteristic of that observation, but more exotic uses are also possible, such as storing lists in rows. Our jerimalai data frame is a good example of a typical data frame, with each row representing a single stone artefact (or observation) and each column represented a variable that we recorded for each artefact. So how is a list different from a vector or data frame? A list is a vector that can contain a mixture of any kind of object. We can have a list or several numeric vectors, or a list that includes several numeric, character and logical vectors, or a list of data frames, or a list of lists, and so on. Lists are the most versatile objects in R because we can store anything in them. Chapter XX goes into more detail about working with lists. 

### Mulitiple spreadsheet files

If we have a handful, or hundreds of spreadsheet files, we can take advantage of the same looping concept that we used for reading multiple sheets to quickly read all these files into our R session. In the example below we have a folder called many_excel_files that contains two Excel files (but it could be any number, 10, 100, 1000 or more files). We will make a character vector of the Excel file names, then we will loop over each file name to extract the data from each Excel file, one at a time, and collect the results in a list:

```{r manyexcelfiles}
# get the file names and store in a character vector
my_numerous_excel_files_names <- list.files("data/many_excel_files", 
                                      full.names = TRUE)

# apply the read_excel() function to each file in our vector of file names
my_numerous_excel_files_data <- map(my_numerous_excel_files_names, 
                                    ~read_excel(.x))
```

In this example we see the function `list.files()`, which is very useful, and comes built-in with R (we don't need to install a package for this one). As we use it here, `list.files()` creates a list of all the files in the many_excel_files folder. But sometimes we don't want to read in all the files, just a set of them, according to something they have in common. We can use the `pattern` argument to restrict the files that `list.files()` will list. For example, if we have a mix of text files and Excel files in a folder, but we only want to get a list of the Excel files, we can use pattern like this:

```{r listfiles}
only_my_excel_files <- list.files("data/many_excel_files", 
                                  pattern = ".xlsx$", 
                                  full.names = TRUE, 
                                  ignore.case = TRUE)
```

When we use `pattern = ".xlsx$"`, we are telling the computer that we only want to list files that have '.xlsx' at the end of their filename (the dollar sign indicates the end of the filename). We can capture files ending with '.xlsx' or '.XLSX' by adding `ignore.case = TRUE`, which will ignore differences between upper and lower case characters. We can also use `pattern` to match other parts of the file names using regular expressions. For example, if we have many spreadsheets that are named in this format: 'region_name-site_name-square_name-recorder_initials.xlsx', we can write R code to select files that have the characters 'BM' only after the third '-' to get all the files for regions/sites/squares that I recorded. We will discuss how to use regular expressions to do this kind of selection in Chapter XX. 

Our use of `map()` in the above example is very similar to how we used it when we read in multiple sheets from one Excel file. We can see the common elements `~` (indicating the function to be repeated) and `.x` (a place-holder for each element of the vector that the function will operate on) in both examples. Similarly, the output we receive from the `map()` function in this example is a list of data frames. This approach is especially useful when we have a large number of Excel files that are similarly structured (i.e. the same number of columns with the same column names) because then we can also use `map()` to analyse the visualise that data.

### Importing messy spreadsheets, or when is it ok to manually edit the data?

In the examples so far, we have assumed that the data in the spreadsheet is a tidy rectangle in the upper left corner of the sheet, and does not contain any formatting or formulas that are important for our analyses. We introduced `skip` as a method to ignore rows with merged cells or other information that we don't need. However, what can we do if our Excel file contains a set of tables in a single sheet? Or when some bold text in some cells conveys important important information? The rexcel package by Rich FitzJohn and Jenny Bryan and the tidyxl package by Duncan Garmonsway can help with capturing data, formulae and formatting information from an Excel sheet. The jailbreakr package, also by Rich FitzJohn and Jenny Bryan can help with Excel files that contains a set of tables laid out on a single sheet. Similarly, the unpivotr package by Duncan Garmonsway can help with importing Excel spreadsheets containing complex or irregular layouts of into regular data frames in R. 

The use of these packages is difficult to generalise into a simple example because messy spreadsheets are often messy in their own way. Here I have contrived an example of a messy spreadsheet with three tables in one sheet \@ref(fig:messyexcel). The data come from Marwick et al. [-@MarwickVanVlack], and the code below shows how we can use the rexcel and jailbreakr packages to automatically identify the three minitables in the sheet, and extract them into a data frame for further work. 

```{r messyexcel, echo = FALSE, out.width = "400pt", fig.cap="Screenshot of a messy Excel sheet with three minitables on one sheet"}
knitr::include_graphics("images/messy_ktc_data.png")
```

In the code below, we use the `rexcel_read()` function to extract data, formatting and formulars from the spreadsheet into R. Then we use the `split_sheet()` function to classify our sheet into regions that contain tabular data. The output from `split_sheet()` is a list, and we can access the second item in the list (i.e. the secon minitable on the sheet) with `messy_ktc_split_sheet[[2]]` (for more about working with lists, see Chapter XX). To access the cell values for this second minitable, we use `messy_ktc_split_sheet[[2]]$values()`, and we apply the function `as.data.frame()` to convert the output to a data frame that we can work with.  

```{r messyspreadsheet}
# to install these packages, you'll need to run these lines:
# devtools::install_github(c("hadley/xml2", 
#                            "rsheets/linen", 
#                            "rsheets/cellranger", 
#                            "rsheets/rexcel", 
#                            "rsheets/jailbreakr"))

library(rexcel)
# read in the messy spreadsheet file
messy_ktc <- rexcel_read("data/messy_ktc_data.xlsx")

library(jailbreakr)
# automatically detect minitables within the sheet
messy_ktc_split_sheet <- split_sheet(messy_ktc)

# access one of the minitables
lithic_data <- as.data.frame(messy_ktc_split_sheet[[2]]$values())
```

Although these methods can make working with messy spreadsheets less painful, they raise the question of whether we should simply edit the spreadsheet manually so that it's more convienent for importing into R (e.g. moving the each of the tables onto their own sheet or own file, or creating a new column to represent the variable that was indicated by the bold text formatting). Manually editing is tempting because it could be quicker and less frustrating than importing the file as-is and write R code to arrange the data into a more useful form. 

The decision about whether to manually edit the data, or write code to arrange the data can be difficult. In my experience, it can take up to an hour or two to work out the code to take data from a messy spreadsheet and arrange it into a convienent form in R. On the other hand, it can take just a few minutes to manually edit the spreadsheet to prepare it for importing into R and be ready for use. If time is the only consideration, the choice may be obvious. However, manually editing the data is problematic because it can involve decisions that change the results of your analysis, but leave no trace and are impossible, or very inconvienent, to reverse. Making undocumented decisions in the data analysis workflow violates a key principle of reproducible research, that every decision about the data analysis should be documented in the analysis scripts. One way around this is to write a brief note that describes how you altered the data from its original form to the form you used to import into R. This note is included with the original, unaltered data, your new modified data in your project compendium so anyone can trace your steps in editing the data. There are some software packages that can help with this (e.g. <https://datproject.org/>), but I have not seen any in regular use among social scientists, so I am reluctant to make specific recommendations. 

One factor that might help you decide where to manually edit a spreadsheet or to write code to workaround the messy spreadsheet is whether or not other people need to edit the spreadsheet during to time that you are working with it. If your collaborators are still updating and circulating the spreadsheet while you are working on the analysis, then it may be disruptive if you reorganise the sheet in the middle of this process. In this situation it may be preferable to write code to handle the messiness. On the other hand, if the data in the spreadsheet is final and no further changes are being made by other people, then manual editing wont be disruptive to the collaboration process, and might be a better option. 

### Spreadsheet data that are not from Excel

Although Excel is by far the most popular spreadsheet application, there are other programs in common use for storing and sharing spreadsheet-style data. In this section we demonstrate how to get data into R from Google Sheets (a web-based collaborative spreadsheet editing program) and SPSS (a commercial software for statitical analysis). 


```{r googlesheetsbasic}
library(googlesheets)
```





## Getting data from other statistical and database software file formats

SPSS haven
STATA
Access  RODBC 
SQL 

## Getting data from tables in documents

PDF tabulizer
MS Word docxtractr
HTML rvest, RSelenium 

## Getting spatial data

shapefiles maptools
tables of spatial data into shapefiles




